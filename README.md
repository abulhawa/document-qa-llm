# ğŸ§  Local Document Q&A System

This project is a **fully local, privacy-first system** for question answering over your documents (PDF, DOCX, TXT). It uses semantic search, chunked embeddings, and a local LLM to answer questions accurately â€” either in single-turn (completion) or multi-turn (chat) modes.

---

## ğŸ”§ Architecture Overview

The system is built from modular components:

### âœ… 1. **Embedding Service** (Dockerized or local)
- Uses `intfloat/multilingual-e5-base` or similar model
- Converts document chunks into vector embeddings

### âœ… 2. **Qdrant** (Vector Store)
- Stores and indexes semantic embeddings
- Supports fast top-k retrieval for chunked search

### âœ… 3. **Text-Generation-WebUI (TGW)**
- Hosts your local LLM (e.g., Mistral, GPTQ, GGUF models)
- Accessible via OpenAI-compatible API endpoints
- Supports both chat and completion-style models

### âœ… 4. **Streamlit Frontend**
- Upload and ingest documents
- Ask questions in chat or single-turn mode
- Switch models, temperatures, and view responses

---

## ğŸš€ Features

- ğŸ” **Semantic Search** over your own documents
- ğŸ’¬ **Chat Mode** with memory of prior turns
- ğŸ§  **Completion Mode** for single-shot Q&A
- ğŸ“ Supports PDF, DOCX, and TXT input
- ğŸ“ Real-time ingestion + deduplication
- ğŸ› ï¸ Plug-and-play backend: switch LLMs or embedding models

---

## ğŸ§ª Usage

### ğŸ“¥ Upload Documents
- Upload one file at a time
- Automatically chunked, embedded, and stored in Qdrant

### ğŸ’¬ Ask Questions
- Choose chat or completion mode
- Ask questions about the content
- In chat mode, you can follow up with contextual questions

### ğŸ§  LLM Settings
- Select model, temperature, and mode from the sidebar

---

## ğŸ§° Requirements

- Python 3.10+
- Running Qdrant (Docker or local)
- Running Text-Generation-WebUI with model loaded
- Optional: Dockerized embedding service (can run standalone as well)

---

## ğŸ“Œ Current Status

- âœ… MVP completed with working ingestion, retrieval, LLM connection
- ğŸ”„ Chat and completion modes supported
- ğŸ” Prompt building and chunk retrieval working
- âš ï¸ Streaming is deprioritized for now

---

## ğŸ›£ï¸ Roadmap

- [ ] Show source attribution (filename + page) with answers
- [ ] Multi-file ingestion (folder support)
- [ ] View/manage indexed files
- [ ] Session save/load
- [ ] Offline Docker bundle (Qdrant + Embedding + Streamlit)
