# ğŸ§  Local Document Q&A System

This project is a **fully local, privacy-first document Q&A system**, designed to help you search, explore, and interact with your own documents â€” securely and efficiently. It supports real-time ingestion of PDF, DOCX, and TXT files, applies semantic chunking and vector embedding, and uses a locally hosted LLM for natural-language answers in both single-turn and multi-turn formats.

The system prioritizes **modularity, observability, and full offline support**, making it suitable for personal knowledge bases, secure enterprise settings, or research workflows â€” all without sending data to the cloud.

---

## ğŸ”­ Vision

This system aims to become a **powerful and private Retrieval-Augmented Generation (RAG) engine**, capable of:

- Ingesting large collections of documents across folders
- Answering questions with real-time citations
- Summarizing or comparing multiple documents
- Operating fully offline, powered by local vector DBs and LLMs
- Providing traceability and observability via Phoenix & OpenTelemetry

---

## ğŸ”§ Architecture Overview

The system is built from modular, testable components:

### âœ… 1. **Embedding Service** (Dockerized or local)

- Runs a multilingual model (e.g., `intfloat/multilingual-e5-base`)
- Accepts batch inputs via a local FastAPI server
- Returns dense embeddings for semantic indexing

### âœ… 2. **Qdrant** (Vector Store)

- Stores document chunk embeddings + metadata (filename, page, position)
- Supports efficient top-k retrieval based on similarity
- Used for both retrieval and metadata tracking (checksums, ingestion status)

### âœ… 3. **Text-Generation-WebUI (TGW)**

- Runs your local LLM (e.g., Mistral, GPTQ, GGUF)
- Accessible via OpenAI-compatible API (`/v1/chat/completions` or `/v1/completions`)
- Works in both chat or completion mode

### âœ… 4. **Streamlit Frontend**

- Upload files and folders
- Ask questions and receive cited answers
- Adjust LLM model, temperature, mode
- Switch between chat and completion

### âœ… 5. **Phoenix Tracing**

- Observability layer based on OpenTelemetry + Arize Phoenix
- Captures span metadata for ingestion, embedding, retrieval, and LLM steps
- Uses OpenInference schema for standardized analytics

---

## ğŸš€ Key Features

- ğŸ” **Semantic Search** over local documents
- ğŸ“ **Supports multiple formats**: PDF, DOCX, TXT
- ğŸ’¬ **Chat Mode** (multi-turn)
- ğŸ§  **Completion Mode** (single Q&A)
- ğŸ“ **Multi-file + folder ingestion**, with parallel processing
- ğŸ§¾ **Source attribution** (filename + page or position)
- ğŸ—ƒï¸ **File deduplication** based on checksum
- ğŸ§± **Modular architecture** (easy to swap models or vector DB)
- ğŸ“Š **Tracing and observability** with Phoenix
- ğŸ”’ **Fully local**: no cloud APIs, no internet needed

---

## ğŸ§ª Usage Guide

### ğŸ“¥ Ingest Documents

- Upload one or more files and/or folders
- Files are recursively scanned, chunked, embedded, and indexed
- Ingestion is logged and deduplicated via checksum tracking

### ğŸ’¬ Ask Questions

- Choose between chat or completion mode
- Type natural-language questions (e.g., "What is this contract about?")
- System retrieves the most relevant document chunks and builds a prompt
- LLM answers using local knowledge + sources

### ğŸ§  LLM Controls

- Model, temperature, and mode are adjustable in sidebar
- Supports any LLM with OpenAI-compatible endpoints

---

## ğŸ§° Requirements

- Python 3.10+
- Qdrant running (Docker or native)
- Text-Generation-WebUI running with a loaded model
- Optional: Dockerized embedding service (recommended for speed)
- Phoenix tracing server (optional but highly recommended)

---

## ğŸ“Œ Current Status

- âœ… Ingestion supports mixed file/folder input, with deduplication
- âœ… Modular pipeline orchestrated by `ingestion.py`
- âœ… Phoenix tracing across ingestion and QA flows
- âœ… Vector store: Qdrant only (no SQLite)
- âœ… Source filenames and pages displayed with each answer
- âœ… Batched embedding via API (embedding model is pluggable)
- âœ… Works with both chat and completion LLMs (e.g. Mistral, GPTQ)
- âœ… Query rewriting layer supports clarification and intent extraction
- âš ï¸ Streaming answers (token-by-token) is currently disabled

---

## ğŸ” Query Rewriting (New Feature)

The system includes a **dedicated LLM-based query rewriter** that improves search accuracy by:

- âœ… Detecting vague or ambiguous questions (e.g., â€œWhat about that contract?â€)
- âœ… Asking for clarification when context is missing (e.g., â€œWho is â€˜heâ€™?â€)
- âœ… Rewriting clean questions into compressed, keyword-rich search phrases

### ğŸ”§ How it works:

- All user queries are passed through a **chat-tuned query rewriter**

- The rewriter returns one of:

  ```json
  { "clarify": "Who are you referring to with 'he'?" }
  ```

  or

  ```json
  { "rewritten": "Ali assistant professor work years" }
  ```

- If clarification is needed, the main pipeline halts and returns the message to the user

- If rewritten, the system uses the **rewritten query for retrieval** (embedding), but keeps the **original question for answering**

### ğŸ“Œ Why this matters:

- Reduces retrieval noise from vague or malformed queries
- Enhances accuracy when using local LLMs + vector search
- Handles grammar errors, typos, lack of punctuation, and missing context

### âœ… Tracing Integration

- The `qa_chain` trace includes a "Rewrite Query" span
- It records:
  - Original user query
  - Rewritten form
  - Clarification flag (if applicable)

---

## ğŸ›£ï¸ Roadmap

### âœ… Completed
- [x] Query reformulation (LLM-assisted search enhancement)
- [x] Multi-file and folder ingestion
- [x] Display source attribution (filename + page/location)
- [x] Phoenix tracing with OpenInference spans and metrics
- [x] Batched embedding via API
- [x] Embedder API Dockerized and integrated
- [x] Source deduplication and ordered display
- [x] Full local LLM support (chat and completion)
- [x] Full file path display for private/local use

### ğŸ§© In Progress / Optional Enhancements
- [ ] Add tracing span for chunking step (`split_documents`)
- [ ] Add tracing span or metrics for Qdrant upsert operation

### ğŸ”® Coming Next
- [ ] Summarize multiple documents using map-reduce (batch summarization)
- [ ] Per-document QA mode (single-file workflows)
- [ ] Hybrid retrieval: combine BM25 + dense vectors
- [ ] Reranker: refine top-k chunks using cross-encoder or LLM
- [ ] Named entity extraction (e.g., Gliner)
- [ ] Advanced chunking (semantic, language-based, LLM-aided)
- [ ] Session save/load for chat history and file tracking
- [ ] Indexed file manager (view/delete/reingest)
- [ ] Offline Docker bundle (Streamlit + Qdrant + Embedder)
- [ ] Agent-based workflows for document reasoning

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ app.py                # Streamlit frontend
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ file_loader.py    # PDF/DOCX/TXT loader
â”‚   â”œâ”€â”€ chunking.py       # Text chunking logic
â”‚   â”œâ”€â”€ embeddings.py     # Embedding API wrapper
â”‚   â”œâ”€â”€ vector_store.py   # Qdrant interaction
â”‚   â”œâ”€â”€ query.py          # QA logic (rewriting + retrieval + LLM)
â”‚   â””â”€â”€ query_rewriter.py # LLM-based query rewriting
â”œâ”€â”€ embedder_api_multilingual/
â”‚   â”œâ”€â”€ app.py            # Embedding service API
â”‚   â”œâ”€â”€ config.py         # Model + batching config
â”‚   â”œâ”€â”€ Dockerfile        # Container setup
â”œâ”€â”€ tracing.py            # Phoenix tracer singleton
â”œâ”€â”€ config.py             # Global config + logger
â”œâ”€â”€ ingestion.py          # Ingestion orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

